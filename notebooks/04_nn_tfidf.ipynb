{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co75EhfI0zgk"
      },
      "source": [
        "# 04 – 1‑Layer Neural Network on TF–IDF Features\n",
        "\n",
        "In this notebook we will:\n",
        "1. Load the train/val/test splits for our chosen task (e.g. SP).  \n",
        "2. Apply the same `clean_text()` as before.  \n",
        "3. Fit a TF–IDF vectorizer to the train set and transform all splits.  \n",
        "4. Convert TF–IDF matrices to PyTorch `Dataset`/`DataLoader`.  \n",
        "5. Define a one‑layer MLP: 20 000 → 512 → 2 (softmax) with dropout.  \n",
        "6. Train with Adam optimizer, monitoring train/val loss and F1.  \n",
        "7. Plot training curves and evaluate on test set.  \n",
        "8. Save the trained model.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import joblib\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n"
      ],
      "metadata": {
        "id": "MY9iz3Ve1G38"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook parameters\n",
        "TASK = \"sp\"                 # or \"hh\" / \"bd\"\n",
        "MAX_FEATURES = 20000\n",
        "HIDDEN_DIM = 512\n",
        "DROPOUT = 0.5\n",
        "LR = 1e-3\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 10\n",
        "PATIENCE = 2               # early stopping patience on val F1\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "7ekmg_Xw1KVa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load splits\n",
        "train_df = pd.read_csv(f\"./project_splits/{TASK}_train.csv\")\n",
        "val_df   = pd.read_csv(f\"./project_splits/{TASK}_val.csv\")\n",
        "test_df  = pd.read_csv(f\"./project_splits/{TASK}_test.csv\")\n",
        "\n",
        "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n"
      ],
      "metadata": {
        "id": "qcHpO_Zm1M0n",
        "outputId": "2f0132ef-de5f-4a07-cb67-c2d6ee0e2ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 231929, Val: 52581, Test: 105164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "V_XZfCKS1Tty",
        "outputId": "01bfeaee-e359-486c-f2dc-2be650cc7610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load stopwords and lemmatizer\n",
        "STOP = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define text cleaning function\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    txt = text.lower()\n",
        "    txt = re.sub(r'<[^>]+>', ' ', txt)\n",
        "    txt = re.sub(r'http\\S+|www\\.\\S+', ' ', txt)\n",
        "    txt = re.sub(r'[^a-z\\s]', ' ', txt)\n",
        "    txt = re.sub(r'\\s+', ' ', txt).strip()\n",
        "    tokens = [w for w in txt.split() if w not in STOP]\n",
        "    return ' '.join(lemmatizer.lemmatize(w) for w in tokens)\n",
        "\n",
        "# Apply cleaning to each DataFrame\n",
        "for df in (train_df, val_df, test_df):\n",
        "    df[\"Text\"] = df[\"Text\"].fillna(\"\")           # Fill missing values with empty string\n",
        "    df[\"cleaned\"] = df[\"Text\"].apply(clean_text) # Apply cleaning\n"
      ],
      "metadata": {
        "id": "9HQMHY9G1Wlk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF–IDF vectorization\n",
        "tfidf = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=(1,2))\n",
        "X_train = tfidf.fit_transform(train_df[\"cleaned\"])\n",
        "X_val   = tfidf.transform(val_df[\"cleaned\"])\n",
        "X_test  = tfidf.transform(test_df[\"cleaned\"])\n",
        "\n",
        "y_train = train_df[\"label\"].values\n",
        "y_val   = val_df[\"label\"].values\n",
        "y_test  = test_df[\"label\"].values\n",
        "\n",
        "# Save TF–IDF for reuse\n",
        "joblib.dump(tfidf, f\"models/{TASK}_tfidf_vectorizer.joblib\")\n"
      ],
      "metadata": {
        "id": "AnjMaXaS1ewK",
        "outputId": "2a170568-ac54-401c-8f31-c8173ff9abe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['models/sp_tfidf_vectorizer.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch datasets\n",
        "def make_loader(X, y, batch_size, shuffle=False):\n",
        "    X_tensor = torch.tensor(X.toarray(), dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "    ds = TensorDataset(X_tensor, y_tensor)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "train_loader = make_loader(X_train, y_train, BATCH_SIZE, shuffle=True)\n",
        "val_loader   = make_loader(X_val,   y_val,   BATCH_SIZE)\n",
        "test_loader  = make_loader(X_test,  y_test,  BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "oMdENs4F2fNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the one-layer MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = MLP(MAX_FEATURES, HIDDEN_DIM, DROPOUT).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n"
      ],
      "metadata": {
        "id": "8ovV8jV42f6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with early stopping on val F1\n",
        "best_val_f1 = 0.0\n",
        "patience_counter = 0\n",
        "history = {\"train_loss\": [], \"val_loss\": [], \"val_f1\": []}\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    # Train\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * Xb.size(0)\n",
        "    train_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
        "            logits = model(Xb)\n",
        "            val_loss += criterion(logits, yb).item() * Xb.size(0)\n",
        "            preds.extend(logits.argmax(dim=1).cpu().tolist())\n",
        "            targets.extend(yb.cpu().tolist())\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_f1 = f1_score(targets, preds, average=\"binary\")\n",
        "\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"val_f1\"].append(val_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch} | train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_f1={val_f1:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), f\"models/{TASK}_nn_best.pth\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "0CAZUyzC2izS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training curves\n",
        "epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "fig, ax1 = plt.subplots(figsize=(6,4))\n",
        "ax1.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "ax1.plot(epochs, history[\"val_loss\"],   label=\"Val Loss\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.legend(loc=\"upper right\")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(epochs, history[\"val_f1\"], color=\"C2\", label=\"Val F1\")\n",
        "ax2.set_ylabel(\"Validation F1\")\n",
        "ax2.legend(loc=\"lower right\")\n",
        "plt.title(\"Training & Validation Curves\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pyxNI1kJ2jjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model and evaluate on test set\n",
        "model.load_state_dict(torch.load(f\"models/{TASK}_nn_best.pth\"))\n",
        "model.eval()\n",
        "preds, targets = [], []\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb = Xb.to(DEVICE)\n",
        "        logits = model(Xb)\n",
        "        preds.extend(logits.argmax(dim=1).cpu().tolist())\n",
        "        targets.extend(yb.tolist())\n",
        "\n",
        "print(classification_report(targets, preds, digits=3))\n",
        "cm = confusion_matrix(targets, preds)\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(cm, cmap=\"Blues\")\n",
        "plt.colorbar()\n",
        "plt.xticks([0,1], [\"0\",\"1\"])\n",
        "plt.yticks([0,1], [\"0\",\"1\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "for i in (0,1):\n",
        "    for j in (0,1):\n",
        "        plt.text(j, i, cm[i,j], ha=\"center\", va=\"center\")\n",
        "plt.title(\"Test Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zI0F344T2nAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final model (optionally together with tfidf)\n",
        "torch.save(model.state_dict(), f\"models/{TASK}_nn_final.pth\")\n",
        "print(\"Model saved to models/{TASK}_nn_final.pth\")\n"
      ],
      "metadata": {
        "id": "ZABKR6Kp2pfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}